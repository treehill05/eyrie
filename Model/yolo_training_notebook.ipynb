{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Person Detection Model Training\n",
        "\n",
        "This notebook trains a YOLO model for person detection and position tracking for crowd analysis.\n",
        "\n",
        "## Dataset Requirements\n",
        "- Images with people in various crowd scenarios\n",
        "- YOLO format annotations (class_id x_center y_center width height)\n",
        "- Class 0: person\n",
        "\n",
        "## Output\n",
        "- Trained YOLO model weights\n",
        "- Position data for each detected person\n",
        "- Bounding box coordinates normalized to image dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install ultralytics torch torchvision opencv-python numpy matplotlib pillow\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to access dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset path (update this path to your dataset location)\n",
        "DATASET_PATH = '/content/drive/MyDrive/CrowdProject/dataset'\n",
        "\n",
        "# Verify dataset structure\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"Dataset found at: {DATASET_PATH}\")\n",
        "    print(\"\\nDataset structure:\")\n",
        "    for root, dirs, files in os.walk(DATASET_PATH):\n",
        "        level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "else:\n",
        "    print(f\"Dataset not found at: {DATASET_PATH}\")\n",
        "    print(\"Please upload your dataset to Google Drive and update DATASET_PATH\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create YOLO dataset configuration file\n",
        "config_content = f\"\"\"\n",
        "# YOLO Dataset Configuration for Person Detection\n",
        "path: {DATASET_PATH}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "# Classes\n",
        "nc: 1  # number of classes\n",
        "names: ['person']  # class names\n",
        "\"\"\"\n",
        "\n",
        "# Write config file\n",
        "config_path = '/content/dataset.yaml'\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(f\"Dataset configuration created at: {config_path}\")\n",
        "print(\"\\nConfiguration content:\")\n",
        "print(config_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load YOLO model (using pre-trained YOLOv8)\n",
        "model = YOLO('yolov8n.pt')  # nano version for faster training\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "results = model.train(\n",
        "    data=config_path,\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    device=device,\n",
        "    project='/content/runs/detect',\n",
        "    name='person_detection',\n",
        "    save_period=10,  # Save checkpoint every 10 epochs\n",
        "    patience=20,     # Early stopping patience\n",
        "    lr0=0.01,        # Initial learning rate\n",
        "    lrf=0.01,        # Final learning rate\n",
        "    momentum=0.937,  # SGD momentum\n",
        "    weight_decay=0.0005,  # Weight decay\n",
        "    warmup_epochs=3,      # Warmup epochs\n",
        "    warmup_momentum=0.8,  # Warmup momentum\n",
        "    warmup_bias_lr=0.1,   # Warmup bias learning rate\n",
        "    box=7.5,              # Box loss gain\n",
        "    cls=0.5,              # Class loss gain\n",
        "    dfl=1.5,              # DFL loss gain\n",
        "    augment=True,         # Enable augmentation\n",
        "    hsv_h=0.015,          # Image HSV-Hue augmentation\n",
        "    hsv_s=0.7,            # Image HSV-Saturation augmentation\n",
        "    hsv_v=0.4,            # Image HSV-Value augmentation\n",
        "    degrees=0.0,          # Image rotation (+/- deg)\n",
        "    translate=0.1,        # Image translation (+/- fraction)\n",
        "    scale=0.5,            # Image scale (+/- gain)\n",
        "    shear=0.0,            # Image shear (+/- deg)\n",
        "    perspective=0.0,      # Image perspective (+/- fraction)\n",
        "    flipud=0.0,           # Image flip up-down (probability)\n",
        "    fliplr=0.5,           # Image flip left-right (probability)\n",
        "    mosaic=1.0,           # Image mosaic (probability)\n",
        "    mixup=0.0,            # Image mixup (probability)\n",
        "    copy_paste=0.0,       # Segment copy-paste (probability)\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the trained model\n",
        "metrics = model.val()\n",
        "\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
        "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
        "print(f\"Recall: {metrics.box.mr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test inference on sample images\n",
        "def test_inference(model, image_path):\n",
        "    \"\"\"\n",
        "    Test inference and return person positions\n",
        "    \n",
        "    Returns:\n",
        "    - image with annotations\n",
        "    - list of person positions [x_center, y_center, width, height]\n",
        "    \"\"\"\n",
        "    results = model(image_path, conf=0.5)\n",
        "    \n",
        "    person_positions = []\n",
        "    annotated_image = None\n",
        "    \n",
        "    for result in results:\n",
        "        # Get annotated image\n",
        "        annotated_image = result.plot()\n",
        "        \n",
        "        # Extract person detections (class 0)\n",
        "        boxes = result.boxes\n",
        "        if boxes is not None:\n",
        "            for box in boxes:\n",
        "                # Get class and confidence\n",
        "                cls = int(box.cls[0])\n",
        "                conf = float(box.conf[0])\n",
        "                \n",
        "                if cls == 0 and conf > 0.5:  # Person class with confidence > 0.5\n",
        "                    # Get bounding box coordinates (normalized)\n",
        "                    x_center, y_center, width, height = box.xywh[0].cpu().numpy()\n",
        "                    \n",
        "                    # Convert to image coordinates if needed\n",
        "                    img_height, img_width = result.orig_shape\n",
        "                    x_center_px = x_center * img_width\n",
        "                    y_center_px = y_center * img_height\n",
        "                    width_px = width * img_width\n",
        "                    height_px = height * img_height\n",
        "                    \n",
        "                    person_positions.append({\n",
        "                        'x_center': float(x_center_px),\n",
        "                        'y_center': float(y_center_px),\n",
        "                        'width': float(width_px),\n",
        "                        'height': float(height_px),\n",
        "                        'confidence': float(conf)\n",
        "                    })\n",
        "    \n",
        "    return annotated_image, person_positions\n",
        "\n",
        "# Test on a sample image (replace with your test image path)\n",
        "sample_image_path = '/content/sample_test.jpg'\n",
        "\n",
        "if os.path.exists(sample_image_path):\n",
        "    annotated_img, positions = test_inference(model, sample_image_path)\n",
        "    \n",
        "    print(f\"Detected {len(positions)} persons:\")\n",
        "    for i, pos in enumerate(positions):\n",
        "        print(f\"Person {i+1}: Center=({pos['x_center']:.1f}, {pos['y_center']:.1f}), \"\n",
        "              f\"Size=({pos['width']:.1f}, {pos['height']:.1f}), \"\n",
        "              f\"Confidence={pos['confidence']:.3f}\")\n",
        "    \n",
        "    # Display annotated image\n",
        "    if annotated_img is not None:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Person Detection Results - {len(positions)} persons detected\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Sample image not found. Please upload a test image to test inference.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model_path = '/content/drive/MyDrive/CrowdProject/models/best_person_detection.pt'\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "\n",
        "# Export model to different formats\n",
        "model.export(format='onnx')  # Export to ONNX for faster inference\n",
        "model.export(format='torchscript')  # Export to TorchScript\n",
        "\n",
        "# Copy best model to Google Drive\n",
        "import shutil\n",
        "best_model_path = '/content/runs/detect/person_detection/weights/best.pt'\n",
        "if os.path.exists(best_model_path):\n",
        "    shutil.copy2(best_model_path, model_path)\n",
        "    print(f\"Best model saved to: {model_path}\")\n",
        "    \n",
        "    # Also save the ONNX version\n",
        "    onnx_path = '/content/drive/MyDrive/CrowdProject/models/best_person_detection.onnx'\n",
        "    if os.path.exists('/content/runs/detect/person_detection/weights/best.onnx'):\n",
        "        shutil.copy2('/content/runs/detect/person_detection/weights/best.onnx', onnx_path)\n",
        "        print(f\"ONNX model saved to: {onnx_path}\")\n",
        "else:\n",
        "    print(\"Best model not found. Training may have failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create inference script for the backend\n",
        "inference_script = '''\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class PersonDetector:\n",
        "    def __init__(self, model_path: str, conf_threshold: float = 0.5):\n",
        "        \"\"\"\n",
        "        Initialize the person detector\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to the trained YOLO model\n",
        "            conf_threshold: Confidence threshold for detections\n",
        "        \"\"\"\n",
        "        self.model = YOLO(model_path)\n",
        "        self.conf_threshold = conf_threshold\n",
        "    \n",
        "    def detect_persons(self, image: np.ndarray) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Detect persons in an image and return their positions\n",
        "        \n",
        "        Args:\n",
        "            image: Input image as numpy array\n",
        "            \n",
        "        Returns:\n",
        "            List of dictionaries containing person positions and metadata\n",
        "        \"\"\"\n",
        "        results = self.model(image, conf=self.conf_threshold)\n",
        "        \n",
        "        person_positions = []\n",
        "        \n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            if boxes is not None:\n",
        "                for box in boxes:\n",
        "                    cls = int(box.cls[0])\n",
        "                    conf = float(box.conf[0])\n",
        "                    \n",
        "                    if cls == 0 and conf > self.conf_threshold:  # Person class\n",
        "                        # Get bounding box coordinates\n",
        "                        x_center, y_center, width, height = box.xywh[0].cpu().numpy()\n",
        "                        \n",
        "                        # Convert to image coordinates\n",
        "                        img_height, img_width = result.orig_shape\n",
        "                        x_center_px = x_center * img_width\n",
        "                        y_center_px = y_center * img_height\n",
        "                        width_px = width * img_width\n",
        "                        height_px = height * img_height\n",
        "                        \n",
        "                        person_positions.append({\n",
        "                            'id': len(person_positions),\n",
        "                            'x_center': float(x_center_px),\n",
        "                            'y_center': float(y_center_px),\n",
        "                            'width': float(width_px),\n",
        "                            'height': float(height_px),\n",
        "                            'confidence': float(conf),\n",
        "                            'normalized_x': float(x_center),\n",
        "                            'normalized_y': float(y_center),\n",
        "                            'normalized_width': float(width),\n",
        "                            'normalized_height': float(height)\n",
        "                        })\n",
        "        \n",
        "        return person_positions\n",
        "    \n",
        "    def get_detection_summary(self, person_positions: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Get a summary of detections\n",
        "        \n",
        "        Args:\n",
        "            person_positions: List of person positions\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with detection summary\n",
        "        \"\"\"\n",
        "        if not person_positions:\n",
        "            return {\n",
        "                'total_persons': 0,\n",
        "                'average_confidence': 0.0,\n",
        "                'positions': []\n",
        "            }\n",
        "        \n",
        "        avg_confidence = sum(pos['confidence'] for pos in person_positions) / len(person_positions)\n",
        "        \n",
        "        return {\n",
        "            'total_persons': len(person_positions),\n",
        "            'average_confidence': avg_confidence,\n",
        "            'positions': person_positions,\n",
        "            'timestamp': int(time.time() * 1000)  # milliseconds\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize detector\n",
        "    detector = PersonDetector('best_person_detection.pt')\n",
        "    \n",
        "    # Load test image\n",
        "    image = cv2.imread('test_image.jpg')\n",
        "    \n",
        "    # Detect persons\n",
        "    positions = detector.detect_persons(image)\n",
        "    \n",
        "    # Get summary\n",
        "    summary = detector.get_detection_summary(positions)\n",
        "    \n",
        "    # Print results\n",
        "    print(json.dumps(summary, indent=2))\n",
        "'''\n",
        "\n",
        "# Save inference script\n",
        "inference_path = '/content/drive/MyDrive/CrowdProject/models/person_detector.py'\n",
        "with open(inference_path, 'w') as f:\n",
        "    f.write(inference_script)\n",
        "\n",
        "print(f\"Inference script saved to: {inference_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Structure Requirements\n",
        "\n",
        "Your dataset should be organized as follows:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/\n",
        "│   │   ├── img001.jpg\n",
        "│   │   ├── img002.jpg\n",
        "│   │   └── ...\n",
        "│   ├── val/\n",
        "│   │   ├── img101.jpg\n",
        "│   │   ├── img102.jpg\n",
        "│   │   └── ...\n",
        "│   └── test/\n",
        "│       ├── img201.jpg\n",
        "│       ├── img202.jpg\n",
        "│       └── ...\n",
        "└── labels/\n",
        "    ├── train/\n",
        "    │   ├── img001.txt\n",
        "    │   ├── img002.txt\n",
        "    │   └── ...\n",
        "    ├── val/\n",
        "    │   ├── img101.txt\n",
        "    │   ├── img102.txt\n",
        "    │   └── ...\n",
        "    └── test/\n",
        "        ├── img201.txt\n",
        "        ├── img202.txt\n",
        "        └── ...\n",
        "```\n",
        "\n",
        "## Annotation Format\n",
        "\n",
        "Each `.txt` file should contain one line per object:\n",
        "```\n",
        "class_id x_center y_center width height\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `class_id`: 0 (for person)\n",
        "- All coordinates are normalized (0-1)\n",
        "- `x_center, y_center`: center of bounding box\n",
        "- `width, height`: width and height of bounding box\n",
        "\n",
        "### Example annotation file (img001.txt):\n",
        "```\n",
        "0 0.5 0.3 0.2 0.4\n",
        "0 0.8 0.7 0.15 0.3\n",
        "```\n",
        "\n",
        "This represents two persons in the image.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "1. Upload your dataset to Google Drive\n",
        "2. Update the `DATASET_PATH` in the Colab notebook\n",
        "3. Run the training cells\n",
        "4. The trained model will be saved to Google Drive\n",
        "\n",
        "## Model Output\n",
        "\n",
        "The trained model will output:\n",
        "- Person detection with confidence scores\n",
        "- Bounding box coordinates (both pixel and normalized)\n",
        "- Position data for each detected person\n",
        "\n",
        "## Inference Format\n",
        "\n",
        "The model returns position data in this format:\n",
        "```json\n",
        "{\n",
        "  \"total_persons\": 2,\n",
        "  \"average_confidence\": 0.85,\n",
        "  \"positions\": [\n",
        "    {\n",
        "      \"id\": 0,\n",
        "      \"x_center\": 320.5,\n",
        "      \"y_center\": 240.3,\n",
        "      \"width\": 64.2,\n",
        "      \"height\": 128.4,\n",
        "      \"confidence\": 0.92,\n",
        "      \"normalized_x\": 0.5,\n",
        "      \"normalized_y\": 0.3,\n",
        "      \"normalized_width\": 0.2,\n",
        "      \"normalized_height\": 0.4\n",
        "    }\n",
        "  ],\n",
        "  \"timestamp\": 1640995200000\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Person Detection Model Training\n",
        "\n",
        "This notebook trains a YOLO model for person detection and position tracking for crowd analysis.\n",
        "\n",
        "## Dataset Requirements\n",
        "- Images with people in various crowd scenarios\n",
        "- YOLO format annotations (class_id x_center y_center width height)\n",
        "- Class 0: person\n",
        "\n",
        "## Output\n",
        "- Trained YOLO model weights\n",
        "- Position data for each detected person\n",
        "- Bounding box coordinates normalized to image dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%pip install ultralytics torch torchvision opencv-python numpy matplotlib pillow\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATASET_PATH = '/content/drive/MyDrive/CrowdProject/dataset'\n",
        "\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"Dataset found at: {DATASET_PATH}\")\n",
        "    print(\"\\nDataset structure:\")\n",
        "    for root, dirs, files in os.walk(DATASET_PATH):\n",
        "        level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "else:\n",
        "    print(f\"Dataset not found at: {DATASET_PATH}\")\n",
        "    print(\"Please upload your dataset to Google Drive and update DATASET_PATH\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLO dataset\n",
        "config_content = f\"\"\"\n",
        "# YOLO Dataset Configuration for Person Detection\n",
        "path: {DATASET_PATH}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "# Classes\n",
        "nc: 1  # number of classes\n",
        "names: ['person']  # class names\n",
        "\"\"\"\n",
        "\n",
        "# Write config file\n",
        "config_path = '/content/dataset.yaml'\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(f\"Dataset configuration created at: {config_path}\")\n",
        "print(\"\\nConfiguration content:\")\n",
        "print(config_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOLOv8\n",
        "model = YOLO('yolov8n.pt') \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = model.train(\n",
        "    data=config_path,\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    device=device,\n",
        "    project='/content/runs/detect',\n",
        "    name='person_detection',\n",
        "    save_period=10,  # Save checkpoint every 10 epochs\n",
        "    patience=20,     # Early stopping patience\n",
        "    lr0=0.01,        # Initial learning rate\n",
        "    lrf=0.01,        # Final learning rate\n",
        "    momentum=0.937,  # SGD momentum\n",
        "    weight_decay=0.0005,  # Weight decay\n",
        "    warmup_epochs=3,      # Warmup epochs\n",
        "    warmup_momentum=0.8,  # Warmup momentum\n",
        "    warmup_bias_lr=0.1,   # Warmup bias learning rate\n",
        "    box=7.5,              # Box loss gain\n",
        "    cls=0.5,              # Class loss gain\n",
        "    dfl=1.5,              # DFL loss gain\n",
        "    augment=True,         # Enable augmentation\n",
        "    hsv_h=0.015,          # Image HSV-Hue augmentation\n",
        "    hsv_s=0.7,            # Image HSV-Saturation augmentation\n",
        "    hsv_v=0.4,            # Image HSV-Value augmentation\n",
        "    degrees=0.0,          # Image rotation (+/- deg)\n",
        "    translate=0.1,        # Image translation (+/- fraction)\n",
        "    scale=0.5,            # Image scale (+/- gain)\n",
        "    shear=0.0,            # Image shear (+/- deg)\n",
        "    perspective=0.0,      # Image perspective (+/- fraction)\n",
        "    flipud=0.0,           # Image flip up-down (probability)\n",
        "    fliplr=0.5,           # Image flip left-right (probability)\n",
        "    mosaic=1.0,           # Image mosaic (probability)\n",
        "    mixup=0.0,            # Image mixup (probability)\n",
        "    copy_paste=0.0,       # Segment copy-paste (probability)\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
